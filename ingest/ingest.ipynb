{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest Batch\n",
    "\n",
    "Ingest a batch of experiments into structured numpy arrays and json meta data files into PRP S3\n",
    "\n",
    "See [file formats](https://github.com/braingeneers/braingeneers/wiki/File-Formats) for details on the source file format and [layout](https://github.com/braingeneers/braingeneers/wiki/data-repository-layout) for details on the repository structure.\n",
    "\n",
    "Any .txt files will be added to a \"notes\" field and any .json files will be combined into the releavant metadata.\n",
    "\n",
    "Note: The original Intan code saves the data as float64 which effectively increaes the size of the data by 4x vs its original source as 16 bit samples. The below code is modified to keep things in 16 bit form and as a result does not scale the amplifier_data structure leaving that to the analysis phase. There is still significant overhead added vs. the raw file (82M vs. 44M) but its much better then just storing the float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import struct\n",
    "import glob\n",
    "import json\n",
    "import datetime\n",
    "import pprint\n",
    "import functools\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# NOTE: Must import before we change to the archive directory...\n",
    "from intanutil.read_header import read_header\n",
    "from intanutil.get_bytes_per_data_block import get_bytes_per_data_block\n",
    "from intanutil.read_one_data_block import read_one_data_block\n",
    "from intanutil.notch_filter import notch_filter\n",
    "from intanutil.data_to_result import data_to_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CacheInfo(hits=0, misses=0, maxsize=128, currsize=0)\n",
      "read_data cache cleared.\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"\n",
    "    Reads an Intan Technologies RHD2000 data file generated by the evaluation board GUI.\n",
    "    Returns data in a dictionary in raw sample units\n",
    "    \n",
    "    Adapted from Intan sample code...\n",
    "    Michael Gibson 17 July 2015\n",
    "    Modified Adrian Foy Sep 2018   \n",
    "    \"\"\"\n",
    "\n",
    "    tic = time.time()\n",
    "    fid = open(filename, 'rb')\n",
    "    filesize = os.path.getsize(filename)\n",
    "\n",
    "    header = read_header(fid)\n",
    "\n",
    "    print('{} amplifier channels'.format(header['num_amplifier_channels']))\n",
    "    print('{} auxiliary input channels'.format(header['num_aux_input_channels']))\n",
    "    print('{} supply voltage channels'.format(header['num_supply_voltage_channels']))\n",
    "    print('{} board ADC channels'.format(header['num_board_adc_channels']))\n",
    "    print('{} board digital input channels'.format(header['num_board_dig_in_channels']))\n",
    "    print('{} board digital output channels'.format(header['num_board_dig_out_channels']))\n",
    "    print('{} temperature sensors channels'.format(header['num_temp_sensor_channels']))\n",
    "\n",
    "    # Determine how many samples the data file contains.\n",
    "    bytes_per_block = get_bytes_per_data_block(header)\n",
    "\n",
    "    # How many data blocks remain in this file?\n",
    "    data_present = False\n",
    "    bytes_remaining = filesize - fid.tell()\n",
    "    if bytes_remaining > 0:\n",
    "        data_present = True\n",
    "\n",
    "    if bytes_remaining % bytes_per_block != 0:\n",
    "        raise Exception('Something is wrong with file size : should have a whole number of data blocks')\n",
    "\n",
    "    num_data_blocks = int(bytes_remaining / bytes_per_block)\n",
    "\n",
    "    num_amplifier_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "    num_aux_input_samples = int((header['num_samples_per_data_block'] / 4) * num_data_blocks)\n",
    "    num_supply_voltage_samples = 1 * num_data_blocks\n",
    "    num_board_adc_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "    num_board_dig_in_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "    num_board_dig_out_samples = header['num_samples_per_data_block'] * num_data_blocks\n",
    "\n",
    "    record_time = num_amplifier_samples / header['sample_rate']\n",
    "\n",
    "    if data_present:\n",
    "        print('File contains {:0.3f} seconds of data.  Amplifiers were sampled at {:0.2f} kS/s.'.format(record_time, header['sample_rate'] / 1000))\n",
    "    else:\n",
    "        print('Header file contains no data.  Amplifiers were sampled at {:0.2f} kS/s.'.format(header['sample_rate'] / 1000))\n",
    "\n",
    "    if data_present:\n",
    "        # Pre-allocate memory for data.\n",
    "        print('')\n",
    "        print('Allocating memory for data...')\n",
    "\n",
    "        data = {}\n",
    "        if (header['version']['major'] == 1 and header['version']['minor'] >= 2) or (header['version']['major'] > 1):\n",
    "            data['t_amplifier'] = np.zeros(num_amplifier_samples, dtype=np.int)\n",
    "        else:\n",
    "            data['t_amplifier'] = np.zeros(num_amplifier_samples, dtype=np.uint)\n",
    "\n",
    "        # NOTE: Changed from uint to uint16\n",
    "        data['amplifier_data'] = np.zeros([header['num_amplifier_channels'], num_amplifier_samples], dtype=np.uint16)\n",
    "        data['aux_input_data'] = np.zeros([header['num_aux_input_channels'], num_aux_input_samples], dtype=np.uint)\n",
    "        data['supply_voltage_data'] = np.zeros([header['num_supply_voltage_channels'], num_supply_voltage_samples], dtype=np.uint)\n",
    "        data['temp_sensor_data'] = np.zeros([header['num_temp_sensor_channels'], num_supply_voltage_samples], dtype=np.uint)\n",
    "        data['board_adc_data'] = np.zeros([header['num_board_adc_channels'], num_board_adc_samples], dtype=np.uint)\n",
    "        \n",
    "        # by default, this script interprets digital events (digital inputs and outputs) as booleans\n",
    "        # if unsigned int values are preferred(0 for False, 1 for True), replace the 'dtype=np.bool' argument with 'dtype=np.uint' as shown\n",
    "        # the commented line below illustrates this for digital input data; the same can be done for digital out\n",
    "        \n",
    "        #data['board_dig_in_data'] = np.zeros([header['num_board_dig_in_channels'], num_board_dig_in_samples], dtype=np.uint)\n",
    "        data['board_dig_in_data'] = np.zeros(\n",
    "            [header['num_board_dig_in_channels'], num_board_dig_in_samples], dtype=np.bool)\n",
    "        data['board_dig_in_raw'] = np.zeros(num_board_dig_in_samples, dtype=np.uint)\n",
    "        \n",
    "        data['board_dig_out_data'] = np.zeros(\n",
    "            [header['num_board_dig_out_channels'], num_board_dig_out_samples], dtype=np.bool)\n",
    "        data['board_dig_out_raw'] = np.zeros(num_board_dig_out_samples, dtype=np.uint)\n",
    "\n",
    "        # Read sampled data from file.\n",
    "        print('Reading data from file...')\n",
    "\n",
    "        # Initialize indices used in looping\n",
    "        indices = {}\n",
    "        indices['amplifier'] = 0\n",
    "        indices['aux_input'] = 0\n",
    "        indices['supply_voltage'] = 0\n",
    "        indices['board_adc'] = 0\n",
    "        indices['board_dig_in'] = 0\n",
    "        indices['board_dig_out'] = 0\n",
    "\n",
    "        print_increment = 10\n",
    "        percent_done = print_increment\n",
    "        for i in range(num_data_blocks):\n",
    "            read_one_data_block(data, header, indices, fid)\n",
    "\n",
    "            # Increment indices\n",
    "            indices['amplifier'] += header['num_samples_per_data_block']\n",
    "            indices['aux_input'] += int(header['num_samples_per_data_block'] / 4)\n",
    "            indices['supply_voltage'] += 1\n",
    "            indices['board_adc'] += header['num_samples_per_data_block']\n",
    "            indices['board_dig_in'] += header['num_samples_per_data_block']\n",
    "            indices['board_dig_out'] += header['num_samples_per_data_block']            \n",
    "\n",
    "            fraction_done = 100 * (1.0 * i / num_data_blocks)\n",
    "            if fraction_done >= percent_done:\n",
    "                print('.', end='', flush=True)\n",
    "#                 print('{}% done...'.format(percent_done))\n",
    "                percent_done = percent_done + print_increment\n",
    "\n",
    "        # Make sure we have read exactly the right amount of data.\n",
    "        bytes_remaining = filesize - fid.tell()\n",
    "        if bytes_remaining != 0: raise Exception('Error: End of file not reached.')\n",
    "\n",
    "    # Close data file.\n",
    "    fid.close()\n",
    "\n",
    "    if (data_present):\n",
    "        print('Parsing data...')\n",
    "\n",
    "        # Extract digital input channels to separate variables.\n",
    "        for i in range(header['num_board_dig_in_channels']):\n",
    "            data['board_dig_in_data'][i, :] = np.not_equal(np.bitwise_and(data['board_dig_in_raw'], (1 << header['board_dig_in_channels'][i]['native_order'])), 0)\n",
    "\n",
    "        # Extract digital output channels to separate variables.\n",
    "        for i in range(header['num_board_dig_out_channels']):\n",
    "            data['board_dig_out_data'][i, :] = np.not_equal(np.bitwise_and(data['board_dig_out_raw'], (1 << header['board_dig_out_channels'][i]['native_order'])), 0)\n",
    "\n",
    "        # Scale voltage levels appropriately.\n",
    "        # NOTE: Commented out to reduce size of file by 4x by storing 16 bit ints in the resulting .npy\n",
    "#         data['amplifier_data'] = np.multiply(0.195, (data['amplifier_data'].astype(np.int32) - 32768))      # units = microvolts\n",
    "        data['aux_input_data'] = np.multiply(37.4e-6, data['aux_input_data'])               # units = volts\n",
    "        data['supply_voltage_data'] = np.multiply(74.8e-6, data['supply_voltage_data'])     # units = volts\n",
    "        if header['eval_board_mode'] == 1:\n",
    "            data['board_adc_data'] = np.multiply(152.59e-6, (data['board_adc_data'].astype(np.int32) - 32768)) # units = volts\n",
    "        elif header['eval_board_mode'] == 13:\n",
    "            data['board_adc_data'] = np.multiply(312.5e-6, (data['board_adc_data'].astype(np.int32) - 32768)) # units = volts\n",
    "        else:\n",
    "            data['board_adc_data'] = np.multiply(50.354e-6, data['board_adc_data'])           # units = volts\n",
    "        data['temp_sensor_data'] = np.multiply(0.01, data['temp_sensor_data'])               # units = deg C\n",
    "\n",
    "        # Check for gaps in timestamps.\n",
    "        num_gaps = np.sum(np.not_equal(data['t_amplifier'][1:]-data['t_amplifier'][:-1], 1))\n",
    "        assert num_gaps == 0  # We don't handle missing samples in all our downstream analysis\n",
    "\n",
    "        # Scale time steps (units = seconds).\n",
    "        data['t_amplifier'] = data['t_amplifier'] / header['sample_rate']\n",
    "        data['t_aux_input'] = data['t_amplifier'][range(0, len(data['t_amplifier']), 4)]\n",
    "        data['t_supply_voltage'] = data['t_amplifier'][range(0, len(data['t_amplifier']), header['num_samples_per_data_block'])]\n",
    "        data['t_board_adc'] = data['t_amplifier']\n",
    "        data['t_dig'] = data['t_amplifier']\n",
    "        data['t_temp_sensor'] = data['t_supply_voltage']\n",
    "\n",
    "        # If the software notch filter was selected during the recording, apply the\n",
    "        # same notch filter to amplifier data here.\n",
    "        assert header['notch_filter_frequency'] == 0\n",
    "        if header['notch_filter_frequency'] > 0:\n",
    "            print('Applying notch filter...')\n",
    "\n",
    "            print_increment = 10\n",
    "            percent_done = print_increment\n",
    "            for i in range(header['num_amplifier_channels']):\n",
    "                data['amplifier_data'][i,:] = notch_filter(data['amplifier_data'][i,:], header['sample_rate'], header['notch_filter_frequency'], 10)\n",
    "\n",
    "                fraction_done = 100 * (i / header['num_amplifier_channels'])\n",
    "                if fraction_done >= percent_done:\n",
    "                    print('{}% done...'.format(percent_done))\n",
    "                    percent_done += print_increment\n",
    "    else:\n",
    "        data = [];\n",
    "\n",
    "    # Move variables to result struct.\n",
    "    result = data_to_result(header, data, data_present)\n",
    "\n",
    "    print('Done!  Elapsed time: {0:0.1f} seconds'.format(time.time() - tic))\n",
    "    return result\n",
    "\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def read_data_cached(path):\n",
    "    return read_data(path)\n",
    "\n",
    "print(read_data_cached.cache_info())\n",
    "print(\"read_data cache cleared.\")\n",
    "read_data_cached.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=0, misses=0, maxsize=128, currsize=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_data_cached.cache_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest\n",
    "Run from here down after setting the UUID and Issue to actually ingest and sync up to PRP S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/public/groups/braingeneers/archive\")\n",
    "\n",
    "# Batch = set of folders each with an experiment and all its recordings\n",
    "batch_uuid = \"2019-02-05-v2\"\n",
    "batch_issue = \"https://github.com/braingeneers/internal/issues/13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting batch 2019-02-05-v2\n",
      "Experiment OrganoidTest1 from original/2019-02-05-v2/OrganoidTest1/\n",
      "Sample original/2019-02-05-v2/OrganoidTest1/OganoidTest1_190205_193542.rhd\n",
      "NAME OrganoidTest1\n",
      "Finished ingesting batch.\n"
     ]
    }
   ],
   "source": [
    "print(\"Ingesting batch {}\".format(batch_uuid))\n",
    "\n",
    "os.makedirs(\"derived/{}\".format(batch_uuid), exist_ok=True)\n",
    "\n",
    "batch_metadata = {\n",
    "    \"uuid\": batch_uuid,\n",
    "    \"issue\": batch_issue,\n",
    "    \"notes\": \"\\n\".join([open(f).read() for f in glob.glob(\"original/{}/*.txt\".format(batch_uuid))]),\n",
    "    \"experiments\": []\n",
    "}\n",
    "\n",
    "# Add any metadata from json files to the batch metadata\n",
    "for f in glob.glob(\"original/{}/*.json\".format(batch_uuid)):\n",
    "    batch_metadata.update(json.load(open(f)))\n",
    "    \n",
    "# Ingest each experiment and exhaust in normalized form into derived/<batch_uuid>/\n",
    "for original_path in sorted(glob.glob(\"original/{}/*/\".format(batch_uuid)))[0:1]:\n",
    "\n",
    "    experiment_metadata = {}\n",
    "    \n",
    "    experiment_metadata[\"name\"] = os.path.basename(os.path.normpath(original_path)).replace(\" \",\"-\")\n",
    "    print(\"Experiment {} from {}\".format(experiment_metadata[\"name\"], original_path))\n",
    "    \n",
    "    # Concatenate all *.txt files into a \"notes\" field\n",
    "    experiment_metadata[\"notes\"] = \"\\n\".join(\n",
    "        [open(f).read() for f in glob.glob(\"{}/*.txt\".format(original_path))])\n",
    "        \n",
    "    # Walk through each *.rhd in the experiment sorted alphabetically and therefore by time\n",
    "    experiment_metadata[\"samples\"] = [] \n",
    "    for sample_path in sorted(glob.glob(\"{}/*.rhd\".format(original_path)))[0:1]:\n",
    "\n",
    "        print(\"Sample {}\".format(sample_path))\n",
    "        \n",
    "        # Try reading its *.rhd file stopping if there is an error. Any earlier\n",
    "        # samples in the run will be retained and stored but with an \"error\" field\n",
    "        # in the experiments metadata\n",
    "        try:\n",
    "            data = read_data_cached(sample_path)\n",
    "        except Exception as e:\n",
    "            experiment_metadata[\"error\"] = \"{}: {}\".format(sample_path, str(e))\n",
    "            print(\"ERROR: {}\".format(experiment_metadata[\"error\"]))\n",
    "            break\n",
    "\n",
    "        sample_metadata = {}\n",
    "        \n",
    "        sample_metadata[\"name\"] = os.path.splitext(os.path.basename(sample_path).replace(\" \",\"-\"))[0]\n",
    "\n",
    "        sample_metadata[\"timestamp\"] = datetime.datetime.strptime(\n",
    "            re.findall(r\"_(\\d+_\\d+)\", sample_metadata[\"name\"])[0], \"%y%m%d_%H%M%S\").isoformat()\n",
    "\n",
    "        if \"timestamp\" not in batch_metadata:\n",
    "            batch_metadata[\"timestamp\"] = sample_metadata[\"timestamp\"]\n",
    "\n",
    "        # Add all the Intan metadata - < is a hack to exclude all sample data\n",
    "        sample_metadata.update({k: v for k, v in data.items() if sys.getsizeof(v) < 2048})\n",
    "        \n",
    "        sample_metadata[\"original\"] = sample_path\n",
    "        sample_metadata[\"derived\"] = \"derived/{}/{}.npy\".format(batch_uuid, sample_metadata[\"name\"])\n",
    "\n",
    "        # Save the original un-modified np.uint16 to save space and so there is no loss of accuracy\n",
    "        sample_metadata[\"units\"] = \"µV\"\n",
    "        sample_metadata[\"offset\"] = 32768\n",
    "        sample_metadata[\"scaler\"] = 0.195\n",
    "        np.save(sample_metadata[\"derived\"], data[\"amplifier_data\"])\n",
    "\n",
    "        experiment_metadata[\"samples\"].append(sample_metadata)\n",
    "\n",
    "    # Add any metadata from json files to the experiment metadata\n",
    "    for f in glob.glob(\"{}/*.json\".format(original_path)):\n",
    "        experiment_metadata.update(json.load(open(f)))\n",
    "\n",
    "    # Save the meta data for this experiment\n",
    "    print(\"NAME\", experiment_metadata[\"name\"])\n",
    "    with open(\"derived/{}/{}.json\".format(batch_uuid, experiment_metadata[\"name\"]), \"w\") as f:\n",
    "        json.dump(experiment_metadata, f, sort_keys=True)\n",
    "\n",
    "    batch_metadata[\"experiments\"].append(\"derived/{}/{}.json\".format(\n",
    "        batch_uuid, experiment_metadata[\"name\"]))\n",
    "    \n",
    "with open(\"derived/{}/metadata.json\".format(batch_uuid), \"w\") as f:\n",
    "    json.dump(batch_metadata, f, sort_keys=True)\n",
    "\n",
    "print(\"Finished ingesting batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Mirror ALL BATCHES AND EXPERIMENTS of local to PRP WITH DELETE - DANGER\n",
    "# !aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "#     s3 sync /public/groups/braingeneers/archive/ s3://braingeneers/archive/ \\\n",
    "#     --delete --acl public-read --dryrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove all derived in PRP S3 from this current ingest\n",
    "# !aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "#     s3 rm --recursive s3://braingeneers/archive/derived/{batch_uuid}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: derived/2019-02-05/OganoidTest1_190205_193642.npy to s3://braingeneers/archive/derived/2019-02-05/OganoidTest1_190205_193642.npy\n",
      "upload: derived/2019-02-05/OrganoidTest1.json to s3://braingeneers/archive/derived/2019-02-05/OrganoidTest1.json\n",
      "upload: derived/2019-02-05/OganoidTest1_190205_193542.npy to s3://braingeneers/archive/derived/2019-02-05/OganoidTest1_190205_193542.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1.json to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1.json\n",
      "upload: derived/2019-02-05/OganoidTest1_190205_193842.npy to s3://braingeneers/archive/derived/2019-02-05/OganoidTest1_190205_193842.npy\n",
      "upload: derived/2019-02-05/OganoidTest1_190205_193742.npy to s3://braingeneers/archive/derived/2019-02-05/OganoidTest1_190205_193742.npy\n",
      "upload: derived/2019-02-05/OganoidTest1_190205_193942.npy to s3://braingeneers/archive/derived/2019-02-05/OganoidTest1_190205_193942.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1_190205_194659.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194659.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2.json to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2.json\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1_190205_194759.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194759.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1_190205_194859.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194859.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1_190205_194959.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194959.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1_190205_195059.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1_190205_195059.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate1_190205_195159.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate1_190205_195159.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2_190205_195513.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195513.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2_190205_195613.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195613.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2_190205_195813.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195813.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press.json to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press.json\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2_190205_195913.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195913.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2_190205_195713.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195713.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate2_190205_200013.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate2_190205_200013.npy\n",
      "upload: derived/2019-02-05/PostStimPressure.json to s3://braingeneers/archive/derived/2019-02-05/PostStimPressure.json\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press_190205_202604.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202604.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press_190205_202804.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202804.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press_190205_202904.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202904.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press_190205_202704.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202704.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press_190205_203004.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_203004.npy\n",
      "upload: derived/2019-02-05/OrganoidTestStimulate3Press_190205_203104.npy to s3://braingeneers/archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_203104.npy\n",
      "upload: derived/2019-02-05/PostStimPressure_190205_200737.npy to s3://braingeneers/archive/derived/2019-02-05/PostStimPressure_190205_200737.npy\n",
      "upload: derived/2019-02-05/PostStimPressure_190205_200937.npy to s3://braingeneers/archive/derived/2019-02-05/PostStimPressure_190205_200937.npy\n",
      "upload: derived/2019-02-05/PostStimPressure_190205_200837.npy to s3://braingeneers/archive/derived/2019-02-05/PostStimPressure_190205_200837.npy\n",
      "upload: derived/2019-02-05/PostStimPressure_190205_201037.npy to s3://braingeneers/archive/derived/2019-02-05/PostStimPressure_190205_201037.npy\n",
      "upload: derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171116.npy to s3://braingeneers/archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171116.npy\n",
      "upload: derived/2019-02-05/Propogating_Stimulus_Test.json to s3://braingeneers/archive/derived/2019-02-05/Propogating_Stimulus_Test.json\n",
      "upload: derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171417.npy to s3://braingeneers/archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171417.npy\n",
      "upload: derived/2019-02-05/PostStimPressure_190205_201137.npy to s3://braingeneers/archive/derived/2019-02-05/PostStimPressure_190205_201137.npy\n",
      "upload: derived/2019-02-05/SquareWaveInput.json to s3://braingeneers/archive/derived/2019-02-05/SquareWaveInput.json\n",
      "upload: derived/2019-02-05/Propagation_test_UCSF_MEA_190205_170916.npy to s3://braingeneers/archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_170916.npy\n",
      "upload: derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171016.npy to s3://braingeneers/archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171016.npy\n",
      "upload: derived/2019-02-05/SweepTest.json to s3://braingeneers/archive/derived/2019-02-05/SweepTest.json\n",
      "upload: derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171217.npy to s3://braingeneers/archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171217.npy\n",
      "upload: derived/2019-02-05/SquareWaveInput_190205_190403.npy to s3://braingeneers/archive/derived/2019-02-05/SquareWaveInput_190205_190403.npy\n",
      "upload: derived/2019-02-05/metadata.json to s3://braingeneers/archive/derived/2019-02-05/metadata.json\n",
      "upload: derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171317.npy to s3://braingeneers/archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171317.npy\n",
      "upload: derived/2019-02-05/SquareWaveInput_190205_190203.npy to s3://braingeneers/archive/derived/2019-02-05/SquareWaveInput_190205_190203.npy\n",
      "upload: derived/2019-02-05/SquareWaveInput_190205_190303.npy to s3://braingeneers/archive/derived/2019-02-05/SquareWaveInput_190205_190303.npy\n",
      "upload: derived/2019-02-05/SweepTest_190205_191200.npy to s3://braingeneers/archive/derived/2019-02-05/SweepTest_190205_191200.npy\n",
      "upload: derived/2019-02-05/SweepTest_190205_191100.npy to s3://braingeneers/archive/derived/2019-02-05/SweepTest_190205_191100.npy\n"
     ]
    }
   ],
   "source": [
    "# Mirror the derived data for this batch to PRP S3 WITH delete\n",
    "!aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "    s3 sync /public/groups/braingeneers/archive/derived/'{batch_uuid}' \\\n",
    "    s3://braingeneers/archive/derived/{batch_uuid} \\\n",
    "    --delete --acl public-read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-02-06 07:39:58   76815488 archive/derived/2019-02-05/OganoidTest1_190205_193542.npy\n",
      "2019-02-06 07:39:58   76815488 archive/derived/2019-02-05/OganoidTest1_190205_193642.npy\n",
      "2019-02-06 07:39:59   76815488 archive/derived/2019-02-05/OganoidTest1_190205_193742.npy\n",
      "2019-02-06 07:39:59   76815488 archive/derived/2019-02-05/OganoidTest1_190205_193842.npy\n",
      "2019-02-06 07:39:59   70041728 archive/derived/2019-02-05/OganoidTest1_190205_193942.npy\n",
      "2019-02-06 07:39:58      72970 archive/derived/2019-02-05/OrganoidTest1.json\n",
      "2019-02-06 07:39:58      87840 archive/derived/2019-02-05/OrganoidTestStimulate1.json\n",
      "2019-02-06 07:40:02   76815488 archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194659.npy\n",
      "2019-02-06 07:40:03   76815488 archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194759.npy\n",
      "2019-02-06 07:40:03   76815488 archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194859.npy\n",
      "2019-02-06 07:40:04   76815488 archive/derived/2019-02-05/OrganoidTestStimulate1_190205_194959.npy\n",
      "2019-02-06 07:40:04   76815488 archive/derived/2019-02-05/OrganoidTestStimulate1_190205_195059.npy\n",
      "2019-02-06 07:40:04   16404608 archive/derived/2019-02-05/OrganoidTestStimulate1_190205_195159.npy\n",
      "2019-02-06 07:40:03      87755 archive/derived/2019-02-05/OrganoidTestStimulate2.json\n",
      "2019-02-06 07:40:08   76815488 archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195513.npy\n",
      "2019-02-06 07:40:08   76815488 archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195613.npy\n",
      "2019-02-06 07:40:09   76815488 archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195713.npy\n",
      "2019-02-06 07:40:09   76815488 archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195813.npy\n",
      "2019-02-06 07:40:09   76815488 archive/derived/2019-02-05/OrganoidTestStimulate2_190205_195913.npy\n",
      "2019-02-06 07:40:11   37417088 archive/derived/2019-02-05/OrganoidTestStimulate2_190205_200013.npy\n",
      "2019-02-06 07:40:09      87785 archive/derived/2019-02-05/OrganoidTestStimulate3Press.json\n",
      "2019-02-06 07:40:16  115261568 archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202604.npy\n",
      "2019-02-06 07:40:16  115261568 archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202704.npy\n",
      "2019-02-06 07:40:16  115261568 archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202804.npy\n",
      "2019-02-06 07:40:16  115261568 archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_202904.npy\n",
      "2019-02-06 07:40:18  115261568 archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_203004.npy\n",
      "2019-02-06 07:40:19   62361728 archive/derived/2019-02-05/OrganoidTestStimulate3Press_190205_203104.npy\n",
      "2019-02-06 07:40:16      73178 archive/derived/2019-02-05/PostStimPressure.json\n",
      "2019-02-06 07:40:23  115261568 archive/derived/2019-02-05/PostStimPressure_190205_200737.npy\n",
      "2019-02-06 07:40:23  115261568 archive/derived/2019-02-05/PostStimPressure_190205_200837.npy\n",
      "2019-02-06 07:40:23  115261568 archive/derived/2019-02-05/PostStimPressure_190205_200937.npy\n",
      "2019-02-06 07:40:24  115261568 archive/derived/2019-02-05/PostStimPressure_190205_201037.npy\n",
      "2019-02-06 07:40:27  115261568 archive/derived/2019-02-05/PostStimPressure_190205_201137.npy\n",
      "2019-02-06 07:40:27   57630848 archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_170916.npy\n",
      "2019-02-06 07:40:27   57630848 archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171016.npy\n",
      "2019-02-06 07:40:27   57630848 archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171116.npy\n",
      "2019-02-06 07:40:28   57630848 archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171217.npy\n",
      "2019-02-06 07:40:29   57630848 archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171317.npy\n",
      "2019-02-06 07:40:27    1720448 archive/derived/2019-02-05/Propagation_test_UCSF_MEA_190205_171417.npy\n",
      "2019-02-06 07:40:27      52942 archive/derived/2019-02-05/Propogating_Stimulus_Test.json\n",
      "2019-02-06 07:40:27      46350 archive/derived/2019-02-05/SquareWaveInput.json\n",
      "2019-02-06 07:40:31   76815488 archive/derived/2019-02-05/SquareWaveInput_190205_190203.npy\n",
      "2019-02-06 07:40:31   76815488 archive/derived/2019-02-05/SquareWaveInput_190205_190303.npy\n",
      "2019-02-06 07:40:28   14883968 archive/derived/2019-02-05/SquareWaveInput_190205_190403.npy\n",
      "2019-02-06 07:40:27      30967 archive/derived/2019-02-05/SweepTest.json\n",
      "2019-02-06 07:40:33  115261568 archive/derived/2019-02-05/SweepTest_190205_191100.npy\n",
      "2019-02-06 07:40:33   95846528 archive/derived/2019-02-05/SweepTest_190205_191200.npy\n",
      "2019-02-06 07:40:29        578 archive/derived/2019-02-05/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# List all the derived files uploaded to PRP S3 for this batch of experiments\n",
    "!aws --profile {os.getenv(\"AWS_PROFILE\")} --endpoint {os.getenv(\"AWS_S3_ENDPOINT\")} \\\n",
    "    s3 ls --recursive s3://braingeneers/archive/derived/{batch_uuid}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
